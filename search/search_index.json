{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Trustworthy Knowledge-Driven Artificial Intelligence (TKAI) Lab</p> <p>At the Trustworthy Knowledge Driven Artificial Intelligence (TKAI) Laboratory, we combine research ideas derived from formal methods, linguistics, cognitive science, and machine learning to efficiently build intelligent systems that are trustworthy, ethical, and secure. These models should have a few desired properties, such as robustness and interpretability, such that humans can easily understand the generated information. This would mean designing agents that are expected to learn desirable behavior with minimal supervision and data that are provably stable and generalize to unseen distribution. We believe these objectives can be stably achieved by designing knowledge-guided, neuro-symbolic agents.</p>"},{"location":"geneology/","title":"Academic Geneology","text":""},{"location":"geneology/#academic-geneology-of-dr-ankur-mali","title":"Academic Geneology of Dr. Ankur Mali","text":"<p>Please find Dr. Ankur Mali's academic geneology below. This information was obtained from Dr. C. Lee Giles' webpage, who was the Academic Advisor of Dr. Ankur Mali at Pennsylvania State University. The Mathematics Genealogy Project is a wonderful tool for tracing academic lineage.</p> <p>His students are part of the following legacy.</p> <p>His advisor was Professor C. Lee Giles, Ph.D. University of Arizona.</p> <p>Lee's advisor was Professor Harrison H. Barrett, Ph.D. Harvard University.</p> <p>Barrett\u2019s advisor was Professor R. Victor Jones, Ph.D. University of California, Berkeley.</p> <p>Jones\u2019 advisor was Professor Carson D. Jeffries, Ph.D. Stanford University.</p> <p>Jeffries\u2019 advisor was Professor Felix Bloch, Nobel Laureate, Ph.D. University of Leipzig.</p> <p>Bloch\u2019s advisor was Professor Werner Karl Heisenberg, Nobel Laureate, Ph.D. University of Munich.</p> <p>Heisenberg\u2019s advisor was Professor Arnold Johannes Wilhelm Sommerfeld, Ph.D. University of K\u00f6nigsberg.</p> <p>Sommerfeld studied mathematics and physical sciences and became a theoretical physicist. He is notable for having four of his students, three of his postdocs, and one student's student being awarded the Nobel Prize. He was nominated 81 times for the Nobel Prize but never received it.</p> <p>Sommerfeld\u2019s advisor was Professor C. L. Ferdinand Lindemann, Ph.D. Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg.</p> <p>Lindemann\u2019s advisor was Professor Felix C. Klein, Ph.D. Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn.</p> <pre><code>When dissertation students had two advisors, both are listed.\n</code></pre> <p>Klein\u2019s advisors were Professor Julius Pl\u00fccker, Ph.D. Philipps-Universit\u00e4t Marburg, and Professor Rudolf Lipschitz, Dr. phil. Universit\u00e4t Berlin.</p> <p>Pl\u00fccker\u2019s advisor was Professor Christian Ludwig Gerling, Ph.D. Georg-August-Universit\u00e4t G\u00f6ttingen.</p> <p>Gerling\u2019s advisor was Professor Carl Friedrich Gauss, Ph.D. Universit\u00e4t Helmstedt. </p> <p>Lipschitz's advisors were Professor Gustav Peter Lejeune Dirichlet, honorary Rheinische Friedrich-Wilhelms-Universit\u00e4t Bonn, and Professor Martin Ohm, Dr. phil. Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg.</p> <p>Dirichlet\u2019s advisors were Professor Simeon Denis Poisson, Ph.D., and Professor Jean-Baptiste Joseph Fourier, Ph.D., both Ecole Polytechnique.</p> <p>Poisson's and Fourier's advisor was Professor Joseph Louis Lagrange.</p> <p>This line continues with Euler, both Bernoulli's, and Leibnitz.</p> <p>For more information please see the Mathematics Genealogy Project.</p>"},{"location":"join/","title":"Join Us","text":"<p>The TKAI Lab is seeking highly motivated Ph.D. and undergraduate students to join our multidisciplinary research efforts aimed at addressing foundational and applied challenges in Artificial Intelligence (AI). Our lab's mission is to develop stable, trustworthy, and explainable AI systems inspired by insights from neuroscience, cognitive science, control theory, and formal methods. Below are key areas of research you will have the opportunity to contribute to:</p>"},{"location":"join/#research-focus-areas","title":"Research Focus Areas","text":"<ul> <li> <p>Neuro-Symbolic AI for Trustworthy Systems:     We are developing methods to encode knowledge graphs into artificial neural networks to create robust and interpretable Artificial General Intelligence (AGI). These systems are designed to operate safely in high-risk domains such as medicine and finance. This aligns with our broader goal of building explainable and trustworthy AI systems by combining symbolic reasoning with neural architectures.</p> </li> <li> <p>Theoretical Foundations of AI Models:     Establishing rigorous mathematical frameworks is central to our work. We aim to derive theoretical limits and stability guarantees for AI models, ensuring they function efficiently in polynomial time. This connects to our ongoing efforts to integrate insights from formal methods into AI development for both theoretical rigor and practical reliability.</p> </li> <li> <p>Predictive Coding Architectures:     Inspired by cognitive science, we are designing predictive coding-based architectures for sequential modeling and multi-modal signal encoding. These systems are evaluated on high-stakes domains, such as medicine and cybersecurity, to ensure stability, efficiency, and adaptability.</p> </li> <li> <p>Fairness by Design in AI Systems:     We aim to create computational models that inherently encode fairness metrics during training. These models are designed with prior constraints to ensure fair and unbiased outcomes, addressing critical ethical concerns in AI deployment.</p> </li> <li> <p>Low-Resource Natural Language Processing (NLP):     Our lab is developing systems capable of generating and recognizing languages in low-resource settings. This research focuses on creating efficient and explainable neural architectures that perform well even with limited training data, enabling broader accessibility of AI technologies.</p> </li> <li> <p>Explainability by Design (XAI):     At TKAI, we prioritize explainability in AI systems, both through inherent architectural design and visualization tools. This aligns with our mission to create trustworthy AI systems that are transparent to end-users, fostering confidence in their real-world deployment.</p> </li> </ul>"},{"location":"join/#phd-students","title":"Ph.D. Students","text":"<p>We are looking for Ph.D. students who are passionate about tackling these research challenges and contributing to cutting-edge advancements in AI.</p>"},{"location":"join/#position-requirements","title":"Position Requirements:","text":"<ul> <li>A Master's or Bachelor's degree in fields such as Mathematics, Applied Physics, Statistics, Electrical Engineering, Computer Science/Engineering, Linguistics, or related disciplines.</li> <li>Preferred skills include:</li> <li>Proficiency in at least one programming language (e.g., Python, C/C++).</li> <li>Strong understanding of Artificial Intelligence, Reinforcement Learning, or Linguistics.</li> <li>Knowledge of formal language theory, mathematical modeling, or multi-modal learning is a plus.</li> </ul>"},{"location":"join/#undergraduate-students","title":"Undergraduate Students","text":"<p>Undergraduate students interested in exploring AI and NLP are encouraged to join as volunteers or through independent study.</p>"},{"location":"join/#requirements","title":"Requirements:","text":"<ul> <li>Interest in topics highlighted on the lab\u2019s webpage.</li> <li>Self-motivated with a willingness to tackle challenging problems.</li> <li>Background in AI/NLP, linear algebra, and statistics is preferred but not required.</li> </ul>"},{"location":"join/#how-to-work-with-me","title":"How to Work with Me","text":""},{"location":"join/#getting-started","title":"Getting Started:","text":"<ul> <li>Start as a volunteer to explore research opportunities and establish a working relationship. Depending on performance, funding opportunities may become available. Volunteers should dedicate a minimum of 5\u201310 hours per week, with flexibility based on obligations and learning curves.</li> <li>Alternatively, take course credits or pursue an independent study under my guidance, focusing on a specific challenging research problem.</li> </ul>"},{"location":"join/#instructions-for-graduate-students","title":"Instructions for Graduate Students:","text":"<ul> <li>Graduate students should meet the general requirements and commit additional hours toward thesis work or publishing research articles.</li> <li>Ensure your research aligns with the lab\u2019s focus areas and be prepared to demonstrate your contributions.</li> </ul>"},{"location":"join/#application-instructions","title":"Application Instructions","text":"<p>If you are interested in working with me, your email should include: 1. What: Describe the problems you wish to explore and how they align with the lab\u2019s research agenda. 2. How: Explain how your skills, background, or experience will contribute to the lab\u2019s success. 3. Note: Remember, I can guide you, but I expect you to take ownership of your work.</p> <p>Include your CV and relevant details, along with the specific research topics you are interested in.</p>"},{"location":"join/#things-to-avoid","title":"Things to Avoid","text":"<ul> <li>Asking about funding without prior work or taking relevant courses.</li> <li>Sending generic emails without reading my papers or understanding the lab\u2019s focus. Such emails will not receive a response.</li> </ul> <p>At TKAI, we believe every student brings a unique perspective to solving complex problems. If you take the time to understand our work, explore the field, and approach us with specific research interests, exceptions can always be made for those eager to learn and contribute.</p>"},{"location":"moments/","title":"Moments","text":""},{"location":"moments/#2024","title":"2024","text":"\u2190 \u2192"},{"location":"moments/#2023","title":"2023","text":"\u2190 \u2192"},{"location":"news/","title":"News &amp; Achievements","text":"<p>Welcome to our lab's news page! Here you'll find the latest updates on our research milestones, student achievements, publications, awards, and other exciting developments from the TKAI Lab.</p>"},{"location":"news/#2025","title":"2025","text":""},{"location":"news/#october-2025","title":"October 2025","text":"<p>\ud83d\udcda Research Publications</p> <ul> <li>One paper accepted in Neural Networks - predictive coding survey with Karl Friston and Rajesh Rao</li> </ul>"},{"location":"news/#september-2025","title":"September 2025","text":"<p>\ud83c\udfc6 Conference Acceptances</p> <ul> <li>One paper accepted in NeurIPS 2025 workshop - Circuit Complexity From Physical Constraints: Scaling Limitations of Attention - Congratulations Benjamin!</li> <li>One paper accepted in EMNLP 2025 (main) - \"Investigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Style\" - Congratulations to students and collaborators.</li> </ul> <p>\ud83c\udfa4 Conference Presentations  Rushitha Santhoshi Mamidala presented \"Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT\" at the 19th International Conference on Neuro-Symbolic Learning and Reasoning (NeSy 2025) \u2014 delivered both oral and poster presentations.</p> <p></p>"},{"location":"news/#july-2025","title":"July 2025","text":"<p>\ud83c\udfa4 Invited Presentations</p> <ul> <li>TKAI lab was invited to present work \"Bridging Neural and Symbolic Computation: A Learnability Study of RNNs on Counter and Dyck Languages\" by Thomas Lukasiewicz group university of Vienna</li> </ul> <p>\ud83d\udcdd Conference Papers</p> <ul> <li>Two more papers accepted into NeSy 2025:</li> <li>\"Neuro\u2011Symbolic Data Collection Automata for Training Language Models on Edge Devices\" with colleagues from Drexel University</li> <li>\"Bridging Neural and Symbolic Computation: A Learnability Study of RNNs on Counter and Dyck Languages\" in collaboration with Penn State</li> </ul> <p>\ud83c\udf93 Student Achievement</p> <ul> <li>Rushitha Santhoshi Mamidala's Master's thesis titled \"Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT\" has been accepted to the NeSy 2025 conference for both an oral and a poster presentation.</li> </ul>"},{"location":"news/#june-2025","title":"June 2025","text":"<p>\ud83c\udfa4 Conference Presentations</p> <ul> <li>Dr. Mali and Colleague presented \"Empirical and Computational Advances in Enzyme Kinetics Prediction: A Machine Learning Approach to Catalytic Variability\" in international metabolic engineering society</li> </ul> <p>\ud83c\udfc6 Prestigious Award</p> <ul> <li>Dr. Mali and his colleagues won prestigious \"2025 ARMA rock mechanics research award\" for their paper \"Crustal Permeability Generated Through Microearthquakes is Constrained by Seismic Moment, Nature Communications, 15:2057\"</li> </ul> <p>\ud83c\udf93 PhD Defense Success</p> <ul> <li>Congratulations Dr. Neisarg Dave for successfully defending his PhD Thesis - Dr. Dave was co-advised by Dr. Mali and will join ADP as Research Scientist.</li> </ul>"},{"location":"news/#may-2025","title":"May 2025","text":"<p>\ud83c\udfa4 Panel Participation</p> <ul> <li>Dr. Mali was invited as panelist at USF AMP conference (https://www.theampconference.com/agenda/session/1624886)</li> </ul> <p>\ud83d\udcdd Conference Acceptance</p> <ul> <li>One paper accepted in IEEE International Conference on Smart Computing</li> </ul> <p>\ud83c\udfc6 Student Award</p> <ul> <li>Undergraduate researcher Shrabon Das won USF CSE UG Award</li> </ul>"},{"location":"news/#april-2025","title":"April 2025","text":"<p>\ud83c\udfc6 Student Research Excellence</p> <ul> <li>Abdul-Malik Zekri presented a poster at the 2025 OneUSF Undergraduate Research Conference, won the 2025 Undergraduate Research - General Disciplinary Student Award, and was recognized at the USF Student Research Awards Luncheon.</li> </ul> <p></p> <ul> <li>Abdul-Malik Zekri presented a poster at the 2025 National Conference on Undergraduate Research (NCUR) titled \"Evolutionary Optimization of Biologically-Inspired Mechanistic Input-Output Neural Circuit Models in the Auditory Pathway.\" The project used custom genetic algorithms to evolve spiking neural circuits based on empirical input-output data.</li> </ul> <p></p>"},{"location":"news/#march-2025","title":"March 2025","text":"<p>\ud83c\udfc6 National Scholarship</p> <ul> <li>Abdul-Malik Zekri was awarded the Barry Goldwater Scholarship, a nationally prestigious scholarship recognizing outstanding undergraduate researchers in STEM fields, for work spanning biologically-inspired neural circuit modeling with Dr. Ankur Mali to semi-automated mitochondria segmentation workflows in collaboration with Dr. George Spirou's Auditory Development and Connectomics Lab.</li> </ul>"},{"location":"news/#february-2025","title":"February 2025","text":"<p>\ud83d\udd2c Research Program</p> <ul> <li>Abdul-Malik Zekri was accepted to the Cold Spring Harbor Laboratory (CSHL) Undergraduate Research Program (URP) for Summer 2025. He will work under Dr. Anthony Zador on a project developing a general model of neuromodulation in neural networks.</li> </ul>"},{"location":"news/#january-2025","title":"January 2025","text":"<p>\ud83c\udf89 Student Internships</p> <ul> <li>Yusra Rasool secured an internship position at Nucor as data intern in the Business development Team.</li> <li>Zhanna Sergeeva secured an internship position at Dell as a software Engineer.</li> </ul> <p>Last updated: October 2025 </p>"},{"location":"people/","title":"People","text":"<p>Principal Investigator - Professor Ankur Mali</p> <p>Professor Ankur Mali completed his Ph.D. under Professor Clyde Lee Giles from The Pennsylvania State University in 2022. He works at the intersection of language, memory, and computation\u2014spanning natural language processing, linguistics, and formal language theory.</p> <p>Furthermore, he has also designed approaches to investigate the mysterious success of deep learning in recognizing natural language from a theoretical and empirical perspective. He also works on designing learning algorithms and computational neural architectures guided by theories of the brain. These architectures focus on solving challenges such as continual/lifelong learning, learning with minimal supervision, Reinforcement Learning, and robustness (both in computer vision and natural language processing).</p>"},{"location":"people/#phd-students","title":"PhD Students","text":"Hitesh Vaidya      Continual Learning, Bio-mimetic Machine Learning      loves running, yoga and travelling    Theophilus Amaefuna      Research Assistant, CSE     Predictive coding, Natural Language Processing  Shion Matsumoto      Research Assistant, CSE     Bioinspired learning, physics-informed neural networks    Benjamin Prada      Research Assistant, CSE     Neural program synthesis, expressability &amp; learnability    Sree Rushitha Santhoshi Mamidala      Research Assistant, CSE     Neuro-symbolic Language Models, Natural Language Processing"},{"location":"people/#masters-students","title":"Master's Students","text":"Tahsun Rahman Khan      Research Assistant, CSE     Deep Learning"},{"location":"people/#undergraduate-students","title":"Undergraduate Students","text":"Yusra      Research Assistant, CSE     Addressing challenges in neural network learnability and stability by exploring alternative algorithms to backpropagation    Zhanna      Research Assistant, CSE     Evaluating learnability and stability in deep learning models by investigating alternative approaches to backpropagation and analyzing the effects of input perturbations      Incoming SWE intern at DELL Technologies    Mustafa Mannan      Research Assistant, CSE     Tackling challenges in neural network learnability and stability Shrabon Kumar Das      Research Assistant, CSE     Investigating learnability and expressivity of neural architectures    Abdul-Malik Zekri      Research Assistant, CSE     Studying how neural architectures shape learning and function in bio-inspired systems    Raul Castillo      Research Assistant, CSE     Studying the capabilities of transformers to model automatas"},{"location":"people/#alumni","title":"Alumni","text":""},{"location":"people/#phd-students_1","title":"PhD students","text":"<ul> <li>Neisarg Dave, advised by Dr. C. Lee Giles, Dr. Dan Kifer, Dr. Ankur Mali</li> <li>Haiwen Guan, advised by Dr. C. Lee Giles and Dr. Ankur Mali</li> <li>Alex Bi, advised by Dr. Parisa Shokouhi and Dr. Ankur Mali</li> </ul>"},{"location":"people/#graduate-students","title":"Graduate students","text":"<ul> <li>Thejasvi Valega, advised by Dr. Parisa Shokouhi and Dr. Ankur Mali</li> <li>Sudarshan Nayak, advised by Dr. Dan Kifer, Dr. Parisa Shokouhi, and Dr. Ankur Mali</li> <li>Alfredo Fernandez, advised by Dr. Ankur Mali.</li> <li>Gerardo Wibmer, advised by Dr. Ankur Mali</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>At the TKAI Lab, our mission is to develop stable systems for Artificial Intelligence applications. Our approach integrates insights from neuroscience, control theory, advanced physics, formal methods, and cognitive science. By combining theoretical foundations with empirical validation, we aim to pioneer stability-driven methodologies. Our focus is on creating systems that demand minimal supervision, leverage smaller datasets, and remain computationally efficient. Above all, we strive to ensure these systems are both explainable and trustworthy, paving the way for reliable and transparent AI solutions.</p>"},{"location":"projects/#neuro-mimetic-approaches","title":"Neuro-Mimetic Approaches","text":"<p>At TKAI Lab, we are inspired by the remarkable efficiency and adaptability of biological systems. Our work focuses on integrating neuro-mimetic principles into AI to develop stable, adaptive, and efficient systems capable of tackling real-world challenges. By fusing insights from neuroscience, control theory, and cognitive science, we aim to create AI systems that emulate human-like learning and decision-making.</p>"},{"location":"projects/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>We explore cutting-edge reinforcement learning techniques that emulate the way biological systems learn through interaction and feedback. Our focus is on developing stable, efficient agents that require minimal data and supervision, while still making optimal decisions in complex environments.</p>"},{"location":"projects/#continual-learning","title":"Continual Learning","text":"<p>Taking inspiration from the brain\u2019s ability to learn incrementally, we develop systems that can seamlessly acquire new knowledge without losing previously learned information. This ensures robust adaptability and stability, enabling AI to thrive in dynamic, real-world settings.</p>"},{"location":"projects/#alternative-to-backpropagation","title":"Alternative to Backpropagation","text":"<p>Challenging the limitations of traditional backpropagation, we are pioneering alternative training methods inspired by neurobiological and control-theoretic principles. These approaches aim to reduce computational overhead, improve energy efficiency, and enhance system stability while maintaining or exceeding current performance benchmarks.</p>"},{"location":"projects/#predictive-coding","title":"Predictive Coding","text":"<p>Predictive coding is a cornerstone of our research, drawing from the brain\u2019s ability to minimize prediction errors. This approach enables the design of AI systems that are not only computationally efficient but also adaptive, allowing them to learn and respond in real time with greater precision and stability.</p>"},{"location":"projects/#formal-methods","title":"Formal Methods","text":"<p>At TKAI Lab, we recognize that stability and trustworthiness must be built on a solid theoretical foundation. Formal methods form the backbone of our research, enabling us to ensure that our AI systems are reliable, transparent, and verifiable in both theory and practice.</p>"},{"location":"projects/#theory","title":"Theory","text":"<p>We develop rigorous mathematical frameworks to analyze and guarantee the stability of AI systems. Our theoretical work bridges the gap between foundational research and real-world applications, ensuring our methods are robust under a variety of conditions.</p>"},{"location":"projects/#empirical","title":"Empirical","text":"<p>To validate our theoretical models, we conduct comprehensive empirical studies. These experiments test the applicability and scalability of our methods in real-world scenarios, ensuring that our stability-driven AI systems perform reliably under diverse and challenging conditions.</p>"},{"location":"projects/#ai-x-science","title":"AI + X (Science)","text":"<p>We are committed to harnessing the power of AI to revolutionize other scientific domains. At TKAI Lab, we explore the intersection of AI with physics, biology, cybersecurity, health and cognitive science to address some of the most complex interdisciplinary challenges. Our aim is to create transformative solutions that are explainable, efficient, and trustworthy, pushing the boundaries of both AI and the sciences it touches.</p>"},{"location":"publications/","title":"Publications","text":"<p>Complete list of publications on my Scholar Page</p>"},{"location":"publications/#2024","title":"2024","text":"<ul> <li> <p>Mali, A., Salvatori, T., &amp; Ororbia, A. (2024). Tight Stability, Convergence, and Robustness Bounds for Predictive Coding Networks. arXiv preprint arXiv:2410.04708.</p> </li> <li> <p>Das, S., &amp; Mali, A. (2024). Exploring Learnability in Memory-Augmented Recurrent Neural Networks: Precision, Stability, and Empirical Insights. arXiv preprint arXiv:2410.03154.</p> </li> <li> <p>Dave, N., Kifer, D., Giles, L., &amp; Mali, A. (2024). Precision, Stability, and Generalization: A Comprehensive Assessment of RNNs learnability capability for Classifying Counter and Dyck Languages. arXiv preprint arXiv:2410.03118.</p> </li> <li> <p>Sharma, A. S., Sarkar, N., Chundawat, V., Mali, A. A., &amp; Mandal, M. (2024). Unlearning or concealment? a critical analysis and evaluation metrics for unlearning in diffusion models. arXiv preprint arXiv:2409.05668.</p> </li> <li> <p>Chatterjee, R., Chundawat, V., Tarun, A., Mali, A., &amp; Mandal, M. (2024). A unified framework for continual learning and machine unlearning. arXiv preprint arXiv:2408.11374.</p> </li> <li> <p>Dave, N., Kifer, D., Giles, C. L., &amp; Mali, A. (2024). Investigating Symbolic Capabilities of Large Language Models. arXiv preprint arXiv:2405.13209.</p> </li> <li> <p>Yu, P., Mali, A., Velaga, T., Bi, A., Yu, J., Marone, C., ... &amp; Elsworth, D. (2024). Crustal permeability generated through microearthquakes is constrained by seismic moment. Nature communications, 15(1), 2057.</p> </li> <li> <p>Vaidya, H., Desell, T., Mali, A., &amp; Ororbia, A. (2024). Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps. arXiv preprint arXiv:2402.12465.</p> </li> <li> <p>Ororbia, A., Mali, A., Kohan, A., Millidge, B., &amp; Salvatori, T. (2024). A review of neuroscience-inspired machine learning. arXiv preprint arXiv:2403.18929.</p> </li> <li> <p>Fernandez, A., &amp; Mali, A. (2024). Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU). arXiv preprint arXiv:2402.02790.</p> </li> <li> <p>Dave, N., Kifer, D., Giles, C. L., &amp; Mali, A. (2024). Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network. arXiv preprint arXiv:2402.02627.</p> </li> <li> <p>Stogin, J., Mali, A., &amp; Giles, C. L. (2024). A provably stable neural network Turing Machine with finite precision and time. Information Sciences, 658, 120034.</p> </li> </ul>"},{"location":"publications/#2023","title":"2023","text":"<ul> <li> <p>Bi, A., Velaga, T., Yu, J., Yu, P., Mali, A., Shokouhi, P., ... &amp; Marone, C. (2023). Machine Learning to Connect Permeability Evolution to Microearthquakes in Hydraulic Stimulations for Enhanced Geothermal Systems. AGU23.</p> </li> <li> <p>Zee, T., Ororbia, A. G., Mali, A., &amp; Nwogu, I. (2022). A robust backpropagation-free framework for images. arXiv preprint arXiv:2206.01820.</p> </li> <li> <p>Mali, A., Ororbia, A., Kifer, D., &amp; Giles, L. (2023). On the computational complexity and formal hierarchy of second order recurrent neural networks. arXiv preprint arXiv:2309.14691.</p> </li> <li> <p>Salvatori, T., Mali, A., Buckley, C. L., Lukasiewicz, T., Rao, R. P., Friston, K., &amp; Ororbia, A. (2023). Brain-inspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870.</p> </li> <li> <p>Ororbia, A. G., Mali, A., Kifer, D., &amp; Giles, C. L. (2023, June). Backpropagation-free deep learning with recursive local representation alignment. In Proceedings of the AAAI conference on artificial intelligence (Vol. 37, No. 8, pp. 9327-9335).</p> </li> <li> <p>Borate, P., Rivi\u00e8re, J., Marone, C., Mali, A., Kifer, D., &amp; Shokouhi, P. (2023). Using a physics-informed neural network and fault zone acoustic monitoring to predict lab earthquakes. Nature communications, 14(1), 3693.</p> </li> <li> <p>Ororbia, A., &amp; Mali, A. (2023, May). Active predictive coding: Brain-inspired reinforcement learning for sparse reward robotic control problems. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 3015-3021). IEEE.</p> </li> <li> <p>Shokouhi, P., Borate, P., Riviere, J., Mali, A., &amp; Kifer, D. (2023, May). Physics-guided machine learning for laboratory earthquake prediction. In EGU General Assembly Conference Abstracts (pp. EGU-15437).</p> </li> <li> <p>Ororbia, A., &amp; Mali, A. (2023). The predictive forward-forward algorithm. arXiv preprint arXiv:2301.01452.</p> </li> </ul>"},{"location":"publications/#2022","title":"2022","text":"<ul> <li> <p>Borate, P., Riviere, J., Marone, C., Mali, A., Kifer, D., &amp; Shokouhi, P. (2022, December). A Physics-informed Machine Learning (PIML) Model for Lab Earthquake Prediction using Time-lapse Active Source Ultrasonic Data. In AGU Fall Meeting Abstracts (Vol. 2022, pp. S55A-08).</p> </li> <li> <p>Ororbia, A., &amp; Mali, A. (2022). Convolutional neural generative coding: Scaling predictive coding to natural images. arXiv preprint arXiv:2211.12047.</p> </li> <li> <p>Ororbia, A., &amp; Mali, A. (2022). Backprop-free reinforcement learning with active neural generative coding. In Proceedings of the AAAI Conference on Artificial Intelligence (pp. 29\u201337).</p> </li> <li> <p>Nguyen, K. N., Tang, Z., Mali, A., &amp; Kelly, A. (2022). Like a bilingual baby: The advantage of visually grounding a bilingual language model. arXiv preprint arXiv:2210.05487.</p> </li> <li> <p>Ororbia, A. G., &amp; Mali, A. (2022, June). Backprop-free reinforcement learning with active neural generative coding. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 1, pp. 29-37).</p> </li> <li> <p>Mali, A., Ororbia, A., Kifer, D., &amp; Giles, C. (2022). Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder. In 2022 Data Compression Conference (DCC) (pp. 471-471).</p> </li> <li> <p>Mali, A. A. (2022). Theoretically deriving computational limits of Artificial Neural Networks with bounded precision and time (Doctoral dissertation, Pennsylvania State University).</p> </li> </ul>"},{"location":"publications/#2021","title":"2021","text":"<ul> <li> <p>Mali, A., Ororbia, A., Kifer, D., &amp; Giles, L. (2021). Recognizing Long Grammatical Sequences using Recurrent Networks Augmented with an External Differentiable Stack. In Proceedings of the Fifteenth International Conference on Grammatical Inference (pp. 130\u2013153). PMLR.</p> </li> <li> <p>Ankur Mali, Alexander Ororbia, Daniel Kifer, &amp; C. L. Giles (2021). An Empirical Analysis of Recurrent Learning Algorithms in Neural Lossy Image Compression Systems. 2021 Data Compression Conference (DCC), 356-356.</p> </li> <li> <p>Mali, A., Ororbia, A., Kifer, D., &amp; Giles, L. (2021). Investigating Backpropagation Alternatives when Learning to Dynamically Count with Recurrent Neural Networks. In Proceedings of the Fifteenth International Conference on Grammatical Inference (pp. 154\u2013175). PMLR.</p> </li> <li> <p>Rao, S., Kumar, V., Kifer, D., Giles, C., &amp; Mali, A. (2021). OmniLayout: Room Layout Reconstruction From Indoor Spherical Panoramas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (pp. 3706-3715).</p> </li> <li> <p>Ankur Arjun Mali, Alexander G. Ororbia II, Daniel Kifer, &amp; C. Lee Giles (2021). Recognizing and Verifying Mathematical Equations using Multiplicative Differential Neural Units. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 (pp. 5006\u20135015). AAAI Press.</p> </li> <li> <p>Ankur Arjun Mali, Alexander G. Ororbia II, &amp; C. Lee Giles (2021). A Neural State Pushdown Automata. IEEE Trans. Artif. Intell., 1(3), 193\u2013205.</p> </li> </ul>"},{"location":"publications/#2020-2013","title":"2020 - 2013","text":"<ul> <li> <p>Gopalakrishnan, A., Mali, A., Kifer, D., Giles, L., &amp; Ororbia, A. G. (2019). A neural temporal model for human motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12116-12125).</p> </li> <li> <p>A. Ororbia, A. Mali, C. L. Giles, &amp; D. Kifer (2020). Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations. IEEE Transactions on Neural Networks and Learning Systems, 1-12.</p> </li> <li> <p>Sudeep D. Thepade, Krishnasagar Subhedarpage, Ankur Mali*, &amp; Tushar S. Vaidya (2013). Performance Gain of Content Based Video Retrieval Technique using Intermediate Block Truncation Coding on Different Color Spaces. 2013 International Conference on Communication and Signal Processing, 1017-1020.</p> </li> <li> <p>Thepade, S., Subhedarpage, K., Mali*, ., &amp; Vaidya, T. (2013). Performance Augmentation of Video Retrieval using Even-Odd Videos with Multilevel Block Truncation Coding. International Journal of Computer Applications, 64(9).</p> </li> <li> <p>Thepade, S., Mali, ., &amp; Subhedarpage*, K. (2014). Content Based Video Retrieval using Thepade\u2019s Ternary Block Truncation Coding and Thepade\u2019s Sorted Ternary Block Truncation Coding with Various Color Spaces. International Journal of Emerging Technologies in Computational and Applied Sciences, 8(6), 462\u2013466.</p> </li> <li> <p>A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, &amp; A. G. Ororbia (2019). A Neural Temporal Model for Human Motion Prediction. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 12108-12117).</p> </li> <li> <p>Ororbia, A., Mali, A., Kelly, M., &amp; Reitter, D. (2019). Like a Baby: Visually Situated Neural Language Acquisition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 5127\u20135136).</p> </li> <li> <p>Alexander G. Ororbia II, &amp; Ankur Mali (2019). Biologically Motivated Algorithms for Propagating Local Target Representations. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 (pp. 4651\u20134658). AAAI Press.</p> </li> <li> <p>A. G. Ororbia, A. Mali, J. Wu, S. O\u2019Connell, W. Dreese, D. Miller, &amp; C. L. Giles (2019). Learned Neural Iterative Decoding for Lossy Image Compression Systems. In 2019 Data Compression Conference (DCC) (pp. 3-12).</p> </li> <li> <p>Thepade, S., Subhedarpage, K., Mali*, ., &amp; Vaidya, T. (2013). Color Content Based Video Retrieval using Block Truncation Coding with Different Color Spaces. International Journal of Computer Applications, 64(3).</p> </li> <li> <p>Thepade, S., Subhedarpage, K., &amp; Mali, . (2013). Performance Rise in Content Based Video Retrieval using Multi-level Thepade\u2019s Sorted Ternary Block Truncation Coding with intermediate block videos and even-odd videos. In Advances in Computing, Communications and Informatics , 2013 International Conference on (pp. 962\u2013966).</p> </li> <li> <p>A. Mali, A. G. Ororbia, &amp; C. L. Giles (2020). The Sibling Neural Estimator: Improving Iterative Image Decoding with Gradient Communication. In 2020 Data Compression Conference (DCC) (pp. 23-32).</p> </li> </ul>"}]}